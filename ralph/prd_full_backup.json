{
  "project": "RLQAS for LiH (4-qubit) - Iterative Optimization Test",
  "version": "1.0",
  "description": "A simplified RLQAS project focusing on LiH molecule with 4 qubits for testing Ralph workflow and iterative optimization. This serves as a practical starting point to validate the RLQAS framework before scaling to larger systems.",
  "objectives": [
    {
      "id": "env-setup",
      "description": "Set up quantum computing environment for LiH molecule with 4 qubits using tencirchem for Hamiltonian generation and energy calculations",
      "priority": "high",
      "acceptance_criteria": [
        "Python environment with tencirchem, PyTorch, and Stable-Baselines3 installed",
        "Script to generate LiH Hamiltonian with 4 qubits using tencirchem's built-in methods",
        "Function to calculate exact (FCI) energy for LiH using tencirchem",
        "Documentation on the molecular configuration, active space selection, and qubit mapping"
      ]
    },
    {
      "id": "quantum-environment",
      "description": "Implement basic quantum circuit construction environment with state and action spaces",
      "priority": "high",
      "acceptance_criteria": [
        "State representation: encode current circuit structure, depth, and energy estimate",
        "Action space: {add_Rx, add_Ry, add_Rz, add_CNOT, adjust_parameter, terminate}",
        "Circuit builder: function to construct circuits from action sequences",
        "Energy calculator: function to compute VQE energy for a given circuit"
      ]
    },
    {
      "id": "reward-function",
      "description": "Design and implement multi-objective reward function balancing accuracy and circuit efficiency",
      "priority": "high",
      "acceptance_criteria": [
        "Accuracy reward: based on energy error relative to FCI",
        "Depth penalty: encourages shallower circuits",
        "Gate count penalty: emphasizes reducing two-qubit gates",
        "Configurable weights for different reward components",
        "Function to compute total reward given circuit and energy"
      ]
    },
    {
      "id": "rl-agent",
      "description": "Implement PPO-based RL agent integrated with the quantum environment",
      "priority": "medium",
      "acceptance_criteria": [
        "PPO implementation using Stable-Baselines3 or custom implementation",
        "Neural network policy with appropriate architecture for circuit decisions",
        "Training loop that connects agent to quantum environment",
        "Experience replay buffer and training logging"
      ]
    },
    {
      "id": "initial-training",
      "description": "Run preliminary training on LiH and evaluate basic performance",
      "priority": "medium",
      "acceptance_criteria": [
        "Training script that runs for specified number of steps",
        "Logging of rewards, circuit depths, and energy errors during training",
        "Evaluation of best found circuit against HEA and random baselines",
        "Visualization of training progress and circuit structures"
      ]
    },
    {
      "id": "analysis-report",
      "description": "Analyze results and produce summary report of the test run",
      "priority": "low",
      "acceptance_criteria": [
        "Comparison of RL-found circuits with baseline methods",
        "Analysis of reward component contributions",
        "Identification of strengths and limitations",
        "Recommendations for next steps and improvements"
      ]
    }
  ],
  "constraints": {
    "technology_stack": ["python3", "tencirchem>=0.6.0", "stable-baselines3>=1.6", "torch>=1.9", "numpy", "matplotlib"],
    "output_requirements": [
      "Clean, modular code with comments",
      "Comprehensive logging and reproducibility",
      "Visualizations of circuits and training progress",
      "Documentation of design decisions and results"
    ],
    "computational_limits": [
      "Focus only on LiH molecule with 4 qubits",
      "Training limited to reasonable time (hours, not days)",
      "Use simplified but meaningful reward function",
      "Prioritize workflow validation over optimal performance"
    ],
    "hardware_considerations": [
      "Quantum simulation with tencirchem will run on CPU",
      "RL training may benefit from GPU but should work on CPU for this small-scale test",
      "Code should be structured to optionally use GPU if available",
      "NEVER run computationally intensive tasks on login nodes (curie-login)",
      "Use SLURM job submission for all computational work:",
      "  - Interactive: salloc to get compute node shell",
      "  - Batch: sbatch with provided slurm_batch.sh template",
      "  - Check available partitions with sinfo command",
      "  - Monitor jobs with squeue -u $USER",
      "Example SLURM scripts provided: slurm_interactive.sh, slurm_batch.sh"
    ]
  },
  "success_criteria": {
    "technical": [
      "Complete Ralph workflow execution through all objectives",
      "Functional RL agent that can learn to construct circuits",
      "Demonstrable improvement over random circuit construction",
      "Working reward function that balances multiple objectives"
    ],
    "process": [
      "Iterative development with clear progress tracking",
      "Knowledge captured in AGENTS.md for future reference",
      "Reproducible experiments with saved configurations",
      "Clear path for extension to larger molecules"
    ]
  }
}